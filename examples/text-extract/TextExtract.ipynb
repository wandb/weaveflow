{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238796ab-6d09-4bbb-8b77-7b9a7c897801",
   "metadata": {},
   "source": [
    "## Example overview\n",
    "\n",
    "This notebook shows how to use Weave to build text extraction capabilities using LLMs.\n",
    "\n",
    "It covers:\n",
    "- [x] experimenting with different techniques for text extraction\n",
    "- [x] using LLMs for text extraction\n",
    "- [x] prompt experimentation\n",
    "- [x] rigorous evaluation of text extraction models\n",
    "- [x] model serving and monitoring\n",
    "- [ ] production feedback capture\n",
    "- [x] building datasets from production data\n",
    "- [x] fine-tuning\n",
    "\n",
    "All of the above is tracked and versioned using Weave, and presented in Weave's UI for analysis.\n",
    "\n",
    "### Setup\n",
    "\n",
    "Install the weave package from the weaveflow branch:\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/wandb/weave@weaveflow\n",
    "```\n",
    "\n",
    "Run the prototype UI locally:\n",
    "\n",
    "```\n",
    "weave ui\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afabe6af-3746-46af-9335-6caa8e67d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "import typing\n",
    "import weave\n",
    "from weave import weaveflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e68ae35-cec2-43e5-9047-162aa129d9ba",
   "metadata": {},
   "source": [
    "Everything will be tracked in the following W&B project, which will be auto-created if it doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d042799-0d61-415f-87e9-0c84e333759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'text-extract76'\n",
    "weave.init(PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddb471e-1297-4679-8303-0b7da34ff89a",
   "metadata": {},
   "source": [
    "This example shows extracting fields from \"Articles of Incorporation\" documents (these are legal documents that filed are when a company is formed). The example documents are generated by gpt-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd984de5-0811-4fc9-b689-26bc475adb39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls example_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6c54c-ab3c-4cd4-9e79-3a0a09f44bdf",
   "metadata": {},
   "source": [
    "An example document looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442171d2-523c-4da5-afed-953bdc5ac684",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_doc = open('example_data/Articles_of_Incorporation_Real_Example_3.txt').read()\n",
    "print(example_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920927d0-2abc-43a1-99e2-6f2f84783c2b",
   "metadata": {},
   "source": [
    "The labels look like this. For now we want to extract the company name, and the initial number of stock shares. Only a subset of the example documents have labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561540c1-c578-4382-a3e8-70b65a16db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.load(open(os.path.join(\"example_data\", \"labels.json\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be29380-39e4-4f78-a3f9-610a650f3349",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "We create table with \"example\" and \"label\" columns, and then publish it with weave to start versioning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ecdb7-3cea-4f4a-a672-055d2c4ac965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our dataset\n",
    "def read_dataset():\n",
    "    dataset_rows = []\n",
    "    raw_labels = json.load(open(os.path.join(\"example_data\", \"labels.json\")))\n",
    "    for example_id, label in raw_labels.items():\n",
    "        example = open(os.path.join('example_data', example_id + '.txt')).read()\n",
    "        dataset_rows.append(\n",
    "            {\"id\": example_id, \"example\": example, \"label\": label})\n",
    "    return dataset_rows\n",
    "\n",
    "# Construct and publish to W&B\n",
    "dataset = weaveflow.Dataset(read_dataset())\n",
    "dataset_ref = weave.publish(dataset, \"eval_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca0f35-b984-4e48-b653-f0409d43958a",
   "metadata": {},
   "source": [
    "Click the link printed above to view the published dataset in the UI.\n",
    "\n",
    "### Editing data in the UI\n",
    "\n",
    "There are a few missing labels! Double-click the table cells to edit the data in the UI, and fix the labels. Then press the commit version to commit the changes.\n",
    "\n",
    "To get the fixed dataset back in Python, you can grab the latest version with the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6448ed-59a7-40d7-8a0f-95ae745647f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ref = weave.ref('eval_dataset')\n",
    "dataset_ref.get().rows[2]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d28a904-12e5-4cb1-9920-9c01e9d21f3b",
   "metadata": {},
   "source": [
    "## Tracking function calls with weave.op\n",
    "\n",
    "Annotate a python function with weave.op to keep track of its code, log and log traces of its calls.\n",
    "\n",
    "This is a simple baseline that uses regexes to try to extract the fields we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1852db4-3c58-4cd3-a097-873122698921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def predict_name(doc: str) -> typing.Any:\n",
    "    match = re.search(r'name.*is ([^.]*)(\\.|\\n)', doc)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def predict_shares(doc: str) -> typing.Any:\n",
    "    match = re.search(r'[s]hares.*?([\\d,]+)', doc)\n",
    "    return match.group(1).replace(',', '') if match else None\n",
    "\n",
    "@weave.op()\n",
    "def predict(doc: str) -> typing.Any:\n",
    "    return {\n",
    "        'name': predict_name(doc),\n",
    "        'shares': predict_shares(doc)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a77410-e7bb-4573-95b1-4561976d60e2",
   "metadata": {},
   "source": [
    "Ops behave like normal functions. But their code is captured and versioned, and their calls are logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982a426-3ae6-4718-8351-d12808577566",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(example_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051aba72-8d33-4d1d-98d6-2efa6cde189c",
   "metadata": {},
   "source": [
    "Click the link printed above to see all the calls of our op.\n",
    "\n",
    "If you change the predict function's code by editing it and rerunning the jupyter cell where it's defined, the next time you call it you'll get a new version of the op. Try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907cd19a-728f-49c5-b3cd-7c6c6457793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we iterate through all the rows in the dataset, performing predictions\n",
    "for row in dataset_ref.get().rows:\n",
    "    print(predict(row['example']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224fdd07-8cf2-4b83-8151-ef91d0989812",
   "metadata": {},
   "source": [
    "Go back to the UI to see all the calls we just made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6839cc82-1692-40ef-a522-9a09240a4c6f",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "A \"model\" is simply a combination of data (which can be configuration, trained model weights, or anything else), and code that says how to execute the model.\n",
    "\n",
    "Use the pattern below to construct a model.\n",
    "\n",
    "The `@weave.type()` decorator makes classes that are automatically published and versioned as they are used.\n",
    "- Like python's dataclasses feature, you must annotate attributes with python types.\n",
    "- You can add methods that are weave ops.\n",
    "\n",
    "Inherit from weave.Model to categorize this object as a Model in the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d12d45-914d-4865-8cb1-70a674e419ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.type()\n",
    "class RegexModel(weaveflow.Model):\n",
    "    extract_name: bool\n",
    "    extract_shares: bool\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, doc: str) -> typing.Any:\n",
    "        return {\n",
    "            'name': predict_name(doc) if self.extract_name else None,\n",
    "            'shares': predict_shares(doc) if self.extract_shares else None\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96505cd8-0798-45e5-ba71-b36e1328a03f",
   "metadata": {},
   "source": [
    "You can instantiate @weave.type() objects like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9593bea7-2b4c-4bed-adc4-904fba5ff03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_model = RegexModel(extract_name=False, extract_shares=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f9246-077f-4a99-a395-1c58ba6d198a",
   "metadata": {},
   "source": [
    "And then call methods on them like normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb6655-2f42-4963-82a3-5c14f9660cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_model.predict(example_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee3fe91-e9c8-4d1c-a57b-0f92647b8d74",
   "metadata": {},
   "source": [
    "If we change the model's configuration, or its definition (by changing its code), Weave will ensure that a new\n",
    "version is published. Here we create a new model configuration and use it. There will be two versions of our RegexModel in the UI after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e41e6e2-eb81-43a0-97a3-70b918f2c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_model = RegexModel(extract_name=True, extract_shares=True)\n",
    "regex_model.predict(example_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a24f6e4-2077-4f51-b16a-8dc0566267d8",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluation is used to give us apple-to-apples comparison of models.\n",
    "\n",
    "You can think of evaluation as simply a function that takes a dataset and a model as input, and produces metrics as output.\n",
    "\n",
    "We've defined an evaluation op that computes f1 scores and other metrics for text extraction problems, in the same directory as this notebook.\n",
    "\n",
    "Just call it to run and track the evaluation for our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a8794-3e01-47e2-9dd5-3bc2a2d2da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate_multi_task_f1\n",
    "evaluate_multi_task_f1(dataset_ref, regex_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf217f-48e5-41ab-acc5-11f18a31f107",
   "metadata": {},
   "source": [
    "Click the link for the evaluation op printed above to see the results in the UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb7b505-9cb3-4f4f-a4e9-af5002976ea0",
   "metadata": {},
   "source": [
    "## Using an LLM\n",
    "\n",
    "Here we define a new model that uses OpenAI to extract the fields we want.\n",
    "\n",
    "We use the `from weave.monitoring import openai` openai API wrapper to ensure the actual OpenAI calls are logged, in addition to the calls to our outer predict method.\n",
    "\n",
    "We parameterize our model with the OpenAI model name, and the prompt template to use. So we'll get a new version of our model in the UI if we try with a different OpenAI model, or a different prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9593dc25-3f08-4bb6-9478-b08b7d72669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.type()\n",
    "class OpenaiLLMModel(weaveflow.Model):\n",
    "    model_name: str\n",
    "    prompt_template: str\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, doc: str) -> typing.Any:\n",
    "        import json\n",
    "        from weave.monitoring import openai\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {'role': 'user',\n",
    "                 'content': self.prompt_template.format(doc=doc)}])\n",
    "        result = response['choices'][0]['message']['content']\n",
    "        parsed = json.loads(result)\n",
    "        return {\n",
    "            'name': parsed['name'],\n",
    "            'shares': int(parsed['shares'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d75dfff-f67e-4e4e-96f7-173278f50b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Extract company name (field: name, string) and number of shares (field: shares, int) from the following Articles of Incorporation document, as a json object: {doc}\"\n",
    "model = OpenaiLLMModel('gpt-3.5-turbo', prompt)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a824e77-5994-4c1b-a23b-72fae15b07b3",
   "metadata": {},
   "source": [
    "Let's call the model.\n",
    "\n",
    "Sometimes this model results in an exception, because OpenAI returns invalid json, which we then try to json.loads. If you run the following cell a few times, you'll see this happen.\n",
    "\n",
    "That's ok! Weave will track raised Exceptions from ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc7156a-8dc4-4616-8209-aa9f223e9119",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.predict(example_doc)\n",
    "except Exception as e:\n",
    "    print('Exception:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5485fa-dba6-4099-aa83-4231648fa25f",
   "metadata": {},
   "source": [
    "### Ecosystem\n",
    "\n",
    "We can generalize our model to work with other chat model providers by using weaveflow.ChatModel from weave's ecosystem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4f8d4a-c9c3-4a96-8415-41a874d37cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.type()\n",
    "class LLMModel(weaveflow.Model):\n",
    "    llm: weaveflow.ChatModel\n",
    "    prompt_template: str\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, doc: str) -> typing.Any:\n",
    "        import json\n",
    "        response = self.llm.complete(messages=[\n",
    "            {'role': 'user',\n",
    "             'content': self.prompt_template.format(doc=doc)}])\n",
    "        result = response['choices'][0]['message']['content']\n",
    "        parsed = json.loads(result)\n",
    "        return {\n",
    "            'name': parsed['name'],\n",
    "            'shares': int(parsed['shares'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5399bc7b-c013-4700-9593-0d15ccf8ee10",
   "metadata": {},
   "source": [
    "Now we can use any ChatModel from the ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b8b36-d04d-44b5-aed3-75e9c893eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLMModel(weaveflow.OpenaiChatModel('gpt-3.5-turbo'), prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7795c67-d171-41dd-a998-f396b7fa5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.predict(example_doc)\n",
    "except Exception as e:\n",
    "    print('Exception:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6667f440-3038-4024-a344-0b965462d52f",
   "metadata": {},
   "source": [
    "Now let's evaluate the new model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5149c23f-a56e-4693-ad88-75c9bb2b88b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_multi_task_f1(dataset_ref, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b1d51b-cdd0-46d9-9b77-d1f34d730e71",
   "metadata": {},
   "source": [
    "The LLM model is much more accurate already, and there's plenty we can do from here to improve it.\n",
    "\n",
    "Take a look at the UI. Click the row in the evaluation that was just created to see the details of this run. You'll see a trace of it's execution, which shows that some of the calls failed due to invalid json output.\n",
    "\n",
    "Try to fix the prompt to improve the json output and try again!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d6c264-6c13-493a-a31e-c1b9a09a1bb0",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "\n",
    "Now let's try with gpt-4 to see if it's any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd97a99-dea2-4d52-9bda-39ea65252305",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLMModel(weaveflow.OpenaiChatModel('gpt-4'), prompt)\n",
    "evaluate_multi_task_f1(dataset_ref, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7317323a-45fe-4644-b195-1f50d9c40852",
   "metadata": {},
   "source": [
    "And let's try with llama 7b. We'll use Anyscale for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d171020-87a8-49d3-b888-ef27c23c9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLMModel(weaveflow.AnyscaleChatModel('meta-llama/Llama-2-70b-chat-hf'), prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7798f014-c9ec-478f-8c51-5fcddd6c5298",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(model.predict(example_doc))\n",
    "except Exception as e:\n",
    "    print('Exception:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc42769-58bc-4536-8103-9344356d6bd5",
   "metadata": {},
   "source": [
    "There's a problem here, looks like the Anyscale model includes extra text before the object. Let's try to fix that by changing the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92514ad-7388-4bc8-b78c-a5b16955da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Extract company name (name) and number of shares (shares) from the following Articles of Incorporation document, as a json object. Only include the object in your response, don't say anything else. Doc: {doc}\"\n",
    "model = LLMModel(weaveflow.AnyscaleChatModel('meta-llama/Llama-2-70b-chat-hf'), prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a449643-d4e6-4ab1-9b19-2f2f9ce62b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(example_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188db6d-41ea-4100-b05d-0de187d07782",
   "metadata": {},
   "source": [
    "That's better, now let's evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c153b316-ba7d-4b23-9e63-a3e14217de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_multi_task_f1(dataset_ref, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f940efa2-3a97-4705-aa59-6af2f3abf501",
   "metadata": {},
   "source": [
    "## Production\n",
    "\n",
    "Anyscale is our best model yet! Nice, let's use it in prod!\n",
    "\n",
    "Once Weave ops and types are published, you can run them elsewhere, without having the original code.\n",
    "\n",
    "Note: You can use the `weave serve` and `weave deploy` commands to serve and deploy models. See the Serve.ipynb example.\n",
    "\n",
    "For now, let's go through a production flow in the notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd2453-7a17-4a50-aaaa-2b4651d6b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production examples are provided in the example_data directory\n",
    "prod_examples = glob.glob(os.path.join('example_data', 'aoi*.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa29ab79-dcd5-4f92-a6d2-26eebf932116",
   "metadata": {},
   "source": [
    "Get the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98d2b81-a0d6-4cdb-9aac-5880e24f146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref = weave.ref('LLMModel')\n",
    "model = model_ref.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcefbc1-0c73-454b-aa0c-1e0d3a40a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through more examples, calling predictions on them\n",
    "# Add the \"env\": \"prod\" attribute so we can distinguish prod predictions from dev\n",
    "with weave.attributes({'env': 'prod'}):\n",
    "    for fname in prod_examples:\n",
    "        doc = open(fname).read()\n",
    "        try:\n",
    "            print(model.predict(doc))\n",
    "        except Exception as e:\n",
    "            print(\"Exception: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a2c884-b1cb-4992-bbb8-6b10cbbb82e7",
   "metadata": {},
   "source": [
    "There are a lot of problems, let's go to the UI to see what's going on.\n",
    "\n",
    "Looks like these examples have a more complicated share structure.\n",
    "\n",
    "First we grab the production predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f1c8b-5e2d-4cc5-838c-66f03d7b03ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get any op's runs by calling the .runs() method\n",
    "\n",
    "prod_runs = [r for r in model.predict.runs() if r.attributes.get('env') == 'prod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64749596-856d-41b7-afa0-eb0b61585951",
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in prod_runs:\n",
    "    print(run.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24f9749-e071-42af-ab27-2719a061ef9a",
   "metadata": {},
   "source": [
    "Let's try with gpt-4 and see if we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaac3b4-dfcd-46de-bf27-47ccb977859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Extract company name (field: 'name', string) and total number of shares (field: 'shares', int) from the following Articles of Incorporation document, as a json object. Only include the object in your response, don't say anything else. Doc: {doc}\"\n",
    "model = LLMModel(weaveflow.OpenaiChatModel('gpt-4'), prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad73fb6f-2116-44ee-ae05-47ae36aaae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "# We store these with an attributes so we can easily get them back.\n",
    "# TODO: need a better approach for creating groups of related work.\n",
    "#   - e.g. weave.experiment() could be a context manager that adds attribute {\"experiment\": <id>} to each record\n",
    "with weave.attributes({'purpose': 'labeling'}):\n",
    "    for i, run in enumerate(prod_runs):\n",
    "        try:\n",
    "            result = model.predict(run.inputs['doc'])\n",
    "            print(\"Result: \", result)\n",
    "            new_dataset.append({'id': str(i), 'example': run.inputs['doc'], 'label': result})\n",
    "        except Exception as e:\n",
    "            print(\"Exception: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e5ddba-674e-4e46-b258-5524ab03c969",
   "metadata": {},
   "source": [
    "Looks better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1fe7d-8269-4828-89aa-f5340b9bb0c4",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "Now let's finetune llama7b to see if we can get it to perform like gpt-4.\n",
    "\n",
    "We get all the gpt-4 model calls from above, and then navigate the trace structure to get the actual openai calls.\n",
    "\n",
    "TODO: It'd be nice if we tracked the lineage from fetching from production through to creating the fine-tuning dataset below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa33edd6-e238-4239-af12-1d2740fda17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "model_ref = weave.ref('LLMModel')\n",
    "model = model_ref.get()\n",
    "# We could go annotate these in the UI now. For now just use all of them\n",
    "label_runs = [r for r in model.predict.runs() if r.attributes.get('purpose') == 'labeling']\n",
    "\n",
    "# Iterate through each model call, fetching its first child span, which is the openai call\n",
    "# This is very slow right now because the API does fetches one at a time. TODO: Fix\n",
    "oai_calls = []\n",
    "for r in tqdm.tqdm(label_runs):\n",
    "    oai_calls.append(r.children()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d31e97-d471-457f-b3f2-907ec26a295e",
   "metadata": {},
   "source": [
    "Fine-tune with anyscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c754f2-a88d-4766-a9e4-d3679a13352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for run in oai_calls[:50]:\n",
    "    data.append({'messages': run.inputs['messages'] + [run.output['choices'][0]['message']]})\n",
    "partition_index = int(len(data) * .55)\n",
    "train_rows = data[:partition_index]\n",
    "validate_rows = data[partition_index:]\n",
    "\n",
    "# TODO: support storing both splits within one Dataset\n",
    "train_ref = weave.publish(weaveflow.Dataset(train_rows), 'prodfinetune-train')\n",
    "validate_ref = weave.publish(weaveflow.Dataset(validate_rows), 'prodfinetune-val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3504041c-3888-4f29-b50c-7a81ffb5792a",
   "metadata": {},
   "source": [
    "Now let's fine Llama-2-7b on our prodfinetune dataset.\n",
    "\n",
    "This will take awhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c43794-d270-40c5-ab96-c6c177ea7df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = weaveflow.AnyscaleChatModel('meta-llama/Llama-2-7b-chat-hf')\n",
    "finetuned_model = chat_model.finetune(train_ref, validate_ref, {'n_epochs': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a5944-621f-4cde-8242-05e4b5fbcb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLMModel(finetuned_model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a19d9e-20d3-428d-b544-05510db9d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.predict(example_doc)\n",
    "except Exception as e:\n",
    "    print(\"Exception: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e6e9bb-a82a-4935-9885-b3ac1229152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, run in enumerate(prod_runs):\n",
    "    try:\n",
    "        print(model.predict(run.inputs['doc']))\n",
    "    except Exception as e:\n",
    "        print(\"Exception: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7064faa-bc91-4e7c-8f4f-6c46868e9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_multi_task_f1(dataset_ref, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
