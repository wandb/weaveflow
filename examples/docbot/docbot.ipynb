{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aaa2b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shawn/.pyenv/versions/3.9.7/envs/weaveflow-editable/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import tiktoken\n",
    "import faiss\n",
    "import numpy as np\n",
    "import openai\n",
    "import weave.monitoring.openai as weave_openai\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b325d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to efficiently embed a set of documents using the OpenAI embedding API\n",
    "# This is from langchain\n",
    "\n",
    "embedding_ctx_length = 8191\n",
    "OPENAI_EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "chunk_size = 1000\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def openai_embed(model, input):\n",
    "    return openai.Embedding.create(input = input, model=model)\n",
    "\n",
    "def openai_embed_texts(texts: List[str], embedding_model: str) -> List[List[float]]:\n",
    "    embeddings: List[List[float]] = [[] for _ in range(len(texts))]\n",
    "    tokens = []\n",
    "    indices = []\n",
    "    encoding = tiktoken.model.encoding_for_model(embedding_model)\n",
    "    for i, text in enumerate(texts):\n",
    "        if embedding_model.endswith(\"001\"):\n",
    "            # See: https://github.com/openai/openai-python/issues/418#issuecomment-1525939500\n",
    "            # replace newlines, which can negatively affect performance.\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "        token = encoding.encode(\n",
    "            text,\n",
    "            disallowed_special=\"all\",\n",
    "        )\n",
    "        for j in range(0, len(token), embedding_ctx_length):\n",
    "            tokens += [token[j : j + embedding_ctx_length]]\n",
    "            indices += [i]\n",
    "\n",
    "    batched_embeddings = []\n",
    "    _chunk_size = chunk_size\n",
    "    for i in range(0, len(tokens), _chunk_size):\n",
    "        response = openai_embed(\n",
    "            embedding_model,\n",
    "            input=tokens[i : i + _chunk_size],\n",
    "        )\n",
    "        batched_embeddings += [r[\"embedding\"] for r in response[\"data\"]]\n",
    "\n",
    "    results: List[List[List[float]]] = [[] for _ in range(len(texts))]\n",
    "    num_tokens_in_batch: List[List[int]] = [[] for _ in range(len(texts))]\n",
    "    for i in range(len(indices)):\n",
    "        results[indices[i]].append(batched_embeddings[i])\n",
    "        num_tokens_in_batch[indices[i]].append(len(tokens[i]))\n",
    "\n",
    "    for i in range(len(texts)):\n",
    "        _result = results[i]\n",
    "        if len(_result) == 0:\n",
    "            average = embed_with_retry(\n",
    "                embedding_model,\n",
    "                input=\"\",\n",
    "            )[\"data\"][0][\"embedding\"]\n",
    "        else:\n",
    "            average = np.average(\n",
    "                _result, axis=0, weights=num_tokens_in_batch[i]\n",
    "            )\n",
    "        embeddings[i] = (average / np.linalg.norm(average)).tolist()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17fcee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import typing\n",
    "import tempfile\n",
    "\n",
    "# class FaissIndexType(weave.types.Type):\n",
    "#     instance_classes = [faiss.Index]\n",
    "    \n",
    "#     def save_instance(self, obj, artifact, name):\n",
    "#         with artifact.writeable_file_path(f\"{name}.faissindex\") as write_path:\n",
    "#             faiss.write_index(obj, write_path)\n",
    "\n",
    "#     def load_instance(self, artifact, name, extra):\n",
    "#         with artifact.open(f\"{name}.faissindex\", binary=True) as f:\n",
    "#             return faiss.read_index(f)\n",
    "\n",
    "class Document(typing.TypedDict):\n",
    "    path: str\n",
    "    contents: str\n",
    "\n",
    "@weave.type()\n",
    "class DocumentDataset:\n",
    "    rows: list[Document]\n",
    "\n",
    "@weave.type()\n",
    "class EmbeddingModel:\n",
    "    pass\n",
    "\n",
    "@weave.type()\n",
    "class OpenAIEmbeddingModel(EmbeddingModel):\n",
    "    model: str\n",
    "\n",
    "    # @weave.op()\n",
    "    def embed_texts(self, texts: list[str]) -> List[List[float]]:\n",
    "        return weave.WeaveList(openai_embed_texts(texts, self.model))\n",
    "\n",
    "@weave.type()\n",
    "class FAISSStore:\n",
    "    index: faiss.IndexFlatL2\n",
    "    docs: DocumentDataset\n",
    "    embedding_model: EmbeddingModel\n",
    "\n",
    "    @weave.op()\n",
    "    def search(self, query: str) -> list[Document]:\n",
    "        embedded_query = self.embedding_model.embed_texts([query])[0]\n",
    "        query_vector = np.array([embedded_query], dtype=np.float32)\n",
    "        scores, indices = self.index.search(query_vector, 4)\n",
    "        return [self.docs.rows[i] for i in indices[0]]\n",
    "        \n",
    "        \n",
    "def make_faiss_store(docs: DocumentDataset, embedding_model: EmbeddingModel) -> FAISSStore:\n",
    "    doc_embeddings = embedding_model.embed_texts(weave.WeaveList([d['contents'] for d in docs.rows]))\n",
    "    faiss_index = faiss.IndexFlatL2(len(doc_embeddings[0]))\n",
    "    doc_embeddings_vector = np.array(doc_embeddings, dtype=np.float32)\n",
    "    faiss_index.add(doc_embeddings_vector)\n",
    "    return FAISSStore(faiss_index, docs, embedding_model)\n",
    "    \n",
    "\n",
    "@weave.type()\n",
    "class DocbotModel(weave.Model):\n",
    "    vector_store: FAISSStore\n",
    "    prompt_template: str\n",
    "    model_name: str\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, query: str) -> str:\n",
    "        docs = self.vector_store.search(query)\n",
    "        prompt = self.prompt_template.format(context='\\n\\n'.join([d['contents'] for d in docs]), question=query)\n",
    "        response = weave_openai.ChatCompletion.create(model=self.model_name, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aaf94c7-eb62-4592-88c4-73aa50ef52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get markdown files from our docs repo\n",
    "\n",
    "DOC_DIR = '/Users/shawn/code2/docodile'\n",
    "DOC_SUFFIX = '.md'\n",
    "\n",
    "docs = []\n",
    "for file in Path(DOC_DIR).glob('**/*' + DOC_SUFFIX):\n",
    "    with file.open('r') as f:\n",
    "        docs.append({'path': file.name, 'contents': f.read()})\n",
    "        \n",
    "docs_dataset = DocumentDataset(docs[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a17fd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View project at http://localhost:3000/browse2/shawn/weaveflow-docbot-11\n"
     ]
    }
   ],
   "source": [
    "weave.init('weaveflow-docbot-11')\n",
    "vs = make_faiss_store(docs_dataset, OpenAIEmbeddingModel(\"text-embedding-ada-002\"))\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "model = DocbotModel(vs, prompt_template, 'gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0cf069-21ec-43a8-933f-371c75f26aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published OpDef to http://localhost:3000/browse2/shawn/weaveflow-docbot-11/OpDef/DocbotModel-predict/737f1b88847d5d015171\n",
      "Published Model to http://localhost:3000/browse2/shawn/weaveflow-docbot-11/Model/DocbotModel/c1581b9c98b5245c3107\n",
      "Published OpDef to http://localhost:3000/browse2/shawn/weaveflow-docbot-11/OpDef/FAISSStore-search/e680f857961ffe143f7a\n",
      "Published FAISSStore to http://localhost:3000/browse2/shawn/weaveflow-docbot-11/FAISSStore/FAISSStore/717d48c6503cfb26c447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To run a sweep, you can use the `wandb sweep` command followed by the path to your sweep configuration YAML file or the sweep ID. \\n\\nFor example:\\n```\\nwandb sweep my_sweep.yaml\\n```\\nor\\n```\\nwandb sweep 123abc\\n```\\n\\nYou can also include additional options such as setting the project and entity, specifying a launch config, and more. \\n\\nOnce you have created a sweep, you can use the `wandb agent` command with the sweep ID to generate hyperparameter suggestions from the sweep and train your model.\\n\\nFor example:\\n```\\nwandb agent 123abc\\n```\\n\\nNote that the above information is specific to using the `wandb` library for managing and running sweeps.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict('How do I run a sweep?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
