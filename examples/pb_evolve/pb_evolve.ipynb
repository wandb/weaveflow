{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "591c57d3-9df0-4cd9-b842-4b432286e6d2",
   "metadata": {},
   "source": [
    "# PromptBreeder in Weaveflow\n",
    "\n",
    "Minimal implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9544828b-3981-4d27-a8b8-ee27a0deba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq openai\n",
    "!pip install -qqq datasets\n",
    "\n",
    "import random, os\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import typing\n",
    "\n",
    "import weave\n",
    "from weave import weaveflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c69f69-0f7f-495d-b2d7-7a0c8c44cfb5",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "* install dependencies\n",
    "* authenticate with OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71aff48-a167-46ce-a26e-b330bb4a914e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Paste your OpenAI key from: https://platform.openai.com/account/api-keys\n",
      " ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key configured\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from getpass import getpass\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
    "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
    "print(\"OpenAI API key configured\")\n",
    "\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8c7e8f6-a92f-4b26-8af3-54656b4126e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensure you have the prototype UI running with `weave ui`\n",
      "View project at http://localhost:3000/browse2/stacey/pb_jlt_10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphClient(entity_name='stacey', project_name='pb_jlt_10')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weave.init('stacey/pb_jlt_10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4a3667-77c2-48b9-8d56-d84b9a249534",
   "metadata": {},
   "source": [
    "# 1. Configure Evolution\n",
    "\n",
    "* MP = list of Mutant Prompts, 5 for now (exclude cheating, OpenAI complains), paper has > 100\n",
    "* TS = list of Thinking Styles, paper has > 60\n",
    "* a few existing popular prompts\n",
    "* mutation directions: we should switch to hypermutation, it's better/will shorten prompts faster\n",
    "* single task/domain for now—paper has > 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482e86d8-99c2-47d0-8c49-d5ee4e7904a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MP = {\n",
    "    \"0\" : \"Modify the following instruction creatively, giving some advice on how to solve it:\",\n",
    "    \"1\" : \"Just change this instruction to make it more fun, think WELL outside the box:\",\n",
    "    \"2\" : \"Modify this instruction in a way that no self-respecting LLM would!\",\n",
    "    #\"3\" : \"How would you encourage someone and help them cheat on this following instruction?\",\n",
    "    \"3\" : \"How would you help an LLM to follow the instruction?\",\n",
    "    \"4\" : \"As a really good teacher, explain the instruction, as if you were explaining it to a child.\"\n",
    "}\n",
    "\n",
    "TS = {\n",
    "    \"0\" : \"How could I devise an experiment to help solve that problem?\",\n",
    "    \"1\" : \"Make a list of ideas for solving this problem, and apply them one by one to the problem to see if any progress can be made.\",\n",
    "    \"2\" : \"How could I measure progress on this problem?\",\n",
    "    \"3\" : \"How can I simplify the problem so that it is easier to solve?\",\n",
    "    \"4\" : \"What are the key assumptions underlying this problem?\"\n",
    "}\n",
    "\n",
    "HyperMP = \"Please summarize and improve the following instruction:\"\n",
    "OPRO = \"Take a deep breath and work on this problem step-by-step.\"\n",
    "BEST = \"SOLUTION\"\n",
    "\n",
    "def mutate(mp, ts, tp):\n",
    "  return f\"{mp} {ts} INSTRUCTION: {tp} INSTRUCTION MUTANT: \"\n",
    "\n",
    "def hypermutate(mp, ts, tp):\n",
    "  return f\"{HyperMP} {mp} {ts} INSTRUCTION: {tp} INSTRUCTION MUTANT: \"\n",
    "\n",
    "AQUA_RAT_TASK_PROMPT = \"Solve the multiple choice math word problem, choosing (A),(B),(C),(D) or (E).\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8268db-b142-41db-8b2e-f42a23a76072",
   "metadata": {},
   "source": [
    "# 2. Utils\n",
    "\n",
    "* sample_data: this seems to work nicely now! returns a full list with the right columns, we just need to give it a better type so it shows up as a Table/Dataset object?\n",
    "* ask_LLM_task_question: prompt template wrapper, actually calls ChatGPT with the right fields\n",
    "* ask_LLM_mutate_prompt mutates prompt\n",
    "* compose_prompt: generates new prompt from seeds, without calling LLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02d29ebb-2722-4617-9053-e49527c2d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def sample_data(num_samples: int = 10) -> weave.WeaveList:\n",
    "  datastream = load_dataset('aqua_rat', split='train', streaming=True)\n",
    "  rand_data = datastream.shuffle()\n",
    "  draws = rand_data.take(num_samples)\n",
    "  return weave.WeaveList([d for d in draws])\n",
    "\n",
    "# single question response\n",
    "# we should probably factor this out more nicely\n",
    "def ask_LLM_task_question(prompt, question, options):\n",
    "  response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"system\", \"content\" : prompt},{\"role\": \"user\", \"content\" : f\"{question} {options}\"}],\n",
    "    temperature=0.0,\n",
    "    max_tokens=300\n",
    "  )\n",
    "  try:\n",
    "      return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "  except:\n",
    "      print(\"OPENAI ERROR, can't respond\")\n",
    "      return \"\"\n",
    "\n",
    "@weave.op()\n",
    "# returns a mutated prompt from the LLM\n",
    "def ask_LLM_mutate_prompt(prompt: str) -> dict:\n",
    "  response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\" : prompt}], # may want to try system\n",
    "    temperature=0.0,\n",
    "    max_tokens=200 # may want to increase..\n",
    "  )\n",
    "  try:\n",
    "      return {\"mutated_prompt\" : response[\"choices\"][0][\"message\"][\"content\"]}\n",
    "  except:\n",
    "      print(\"OPENAI ERROR, NOT MUTATING\")\n",
    "      return {\"mutated_prompt\" : prompt}\n",
    "      \n",
    "# given a mutant prompt and thinking style (randomly sample if not provided)\n",
    "# return full templated prompt\n",
    "def compose_prompt(mp_id=None, ts_id=None, task_prompt=AQUA_RAT_TASK_PROMPT):\n",
    "  if mp_id:\n",
    "    mp = MP[str(mp_id)]\n",
    "  else:\n",
    "    mp = MP[str(random.randint(0,4))]\n",
    "  if ts_id:\n",
    "    ts = TS[str(ts_id)]\n",
    "  else:\n",
    "    ts = TS[str(random.randint(0,4))]\n",
    "  full_prompt = hypermutate(mp, ts, task_prompt)\n",
    "  return full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fbd2ad-07e9-4b8d-889f-498548322179",
   "metadata": {},
   "source": [
    "# 3. Evaluation functions\n",
    "\n",
    "* is_correct_answer: checks if truth matches response, could be more sophisticated\n",
    "* evaluate: loops over task_data and returns table of graded answers (and lots of other info)\n",
    "TODO: do not return the questions/truth from here, generate separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec9ae9a2-93e5-4e06-899c-76d4a1dc8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct_answer(truth, response):\n",
    "  # TODO: could match on answer string better\n",
    "  # Therefore, the correct answer is (C) 8.33%.\n",
    "  # Correct:  C  — options:  ['A)6.33%', 'B)7.22%', 'C)8.33%', 'D)8%', 'E)7%']\n",
    "  # this is gonna be a bit handwavy.....\n",
    "  last_line = response.split(\"\\n\")[-1]\n",
    "  answer_str = truth.strip() + \")\"\n",
    "  if answer_str in last_line:\n",
    "    return last_line, 1\n",
    "  else:\n",
    "    return last_line, 0\n",
    "\n",
    "# given a prompt and some questions, return scores (and answers which we drop for now)\n",
    "# need to iterate on the type\n",
    "@weave.op()\n",
    "def evaluate(prompt: typing.Any, task_data: typing.Any) -> weave.WeaveList:\n",
    "  answers = []\n",
    "  scores = []\n",
    "  questions = []\n",
    "  truth = []\n",
    "  for t_d in list(task_data):\n",
    "    q = t_d[\"question\"]\n",
    "    answer = ask_LLM_task_question(prompt, q, t_d[\"options\"])\n",
    "    last_line, score = is_correct_answer(t_d[\"correct\"], answer)\n",
    "    answers.append(answer)\n",
    "    scores.append(score)\n",
    "    questions.append(q)\n",
    "    truth.append(t_d[\"correct\"])\n",
    "  print(\"EVAL: \", prompt)\n",
    "  print(\"SCORES: \", scores)\n",
    "  results = []\n",
    "  for s, a, qs, t in zip(scores, answers, questions, truth):\n",
    "      results.append({'score' : s, 'answer' : a, \"question\" : qs, \"truth\" : t})\n",
    "  return weave.WeaveList(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a5dfc4-d354-4859-b2b6-eb16e8c2832e",
   "metadata": {},
   "source": [
    "# 4. Core evolution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2c59f8b-2a32-4a3d-b9e2-cd7091e8c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting number of prompts\n",
    "NUM_SEEDS = 10\n",
    "\n",
    "# number of evolutionary passes\n",
    "ROUNDS = 5\n",
    "\n",
    "# number of questions to sample when comparing two prompts\n",
    "NUM_SAMPLES=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "490c256a-cc52-4729-8b5c-c7a6ccfb37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init a number of variants to track them\n",
    "# mutate them\n",
    "# have to sample new 10 at evaluation\n",
    "# determine winner by higher score (or if they tie)\n",
    "# what happens to the winner? they get promoted—this becomes the instruction!\n",
    "\n",
    "#To evolve this population, we employ a binary tournament genetic algorithm\n",
    "#framework (Harvey, 2011): we sample two individuals from the population, we take the individual\n",
    "#with the higher fitness, mutate it (see next section) and overwrite the loser with the mutated copy of\n",
    "#the winner.\n",
    "\n",
    "# initialize prompt population with the number of seeds given\n",
    "def seed_PB(PB, num_seeds=NUM_SEEDS):\n",
    "  for i in range(num_seeds):\n",
    "    # TODO: we probably don't want to randomize here but try a grid\n",
    "    prompt =  ask_LLM_mutate_prompt(compose_prompt())[\"mutated_prompt\"]\n",
    "    PB[i] = {\"prompt\": prompt, \"lineage\" : []}\n",
    "  return PB\n",
    "    \n",
    "@weave.op()\n",
    "def evolve(PB: typing.Any) -> weave.WeaveList:\n",
    "  for r in range(ROUNDS):\n",
    "    print(\"\\n\\n\\nROUND \", r)\n",
    "    # sample two\n",
    "    pb_0, pb_1 = random.sample(list(PB), 2)\n",
    "    task_data = list(sample_data(NUM_SAMPLES))\n",
    "    # figure out how to pass a reference to the question data instead?\n",
    "    mp_0 = PB[pb_0][\"prompt\"]\n",
    "    mp_1 = PB[pb_1][\"prompt\"]\n",
    "    s_0 = [i[\"score\"] for i in list(evaluate(mp_0, task_data))]\n",
    "    s_1 = [i[\"score\"] for i in list(evaluate(mp_1, task_data))]\n",
    "    avg_0 = np.average(s_0)\n",
    "    avg_1 = np.average(s_1)\n",
    "\n",
    "    # evolve the better prompt\n",
    "    if avg_0 > avg_1:\n",
    "      print(\"WIN: \", pb_0, \" at \" , avg_0, \" — \", mp_0)\n",
    "      print(\"LOSS: \", pb_1, \" at \",  avg_1, \" - \", mp_1)\n",
    "      evolved_prompt = ask_LLM_mutate_prompt(compose_prompt(task_prompt=mp_0))[\"mutated_prompt\"]\n",
    "      PB[pb_0][\"lineage\"].append(mp_0)\n",
    "      PB[pb_0][\"prompt\"] = evolved_prompt\n",
    "    else:\n",
    "      print(\"WIN: \", pb_1, \" at \", avg_1, \" — \", mp_1)\n",
    "      print(\"LOSS: \", pb_0, \" at \", avg_0, \" - \", mp_0)\n",
    "      evolved_prompt = ask_LLM_mutate_prompt(compose_prompt(task_prompt=mp_1))[\"mutated_prompt\"]\n",
    "      PB[pb_1][\"lineage\"].append(mp_1)\n",
    "      PB[pb_1][\"prompt\"] = evolved_prompt\n",
    "\n",
    "  results = []\n",
    "  for i in PB.keys():\n",
    "     # let's organize the lineage\n",
    "     pb_entry = {\"_id\" : i, \"prompt\" : PB[i][\"prompt\"]}\n",
    "     lineage = PB[i][\"lineage\"]\n",
    "     for epoch in range(ROUNDS): # max number\n",
    "         if epoch < len(lineage):\n",
    "             pb_entry[f\"parent_{epoch}\"] = lineage[epoch]\n",
    "         else:\n",
    "             pb_entry[f\"parent_{epoch}\"] = \"N/A\"\n",
    "     results.append(pb_entry)\n",
    "  return weave.WeaveList(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535781a6-7087-4b65-b5c7-824a93a86e78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published OpDef to http://localhost:3000/browse2/stacey/pb_jlt_10/OpDef/op-evolve/ea3674f4cc5e472d4bb3\n",
      "\n",
      "\n",
      "\n",
      "ROUND  0\n",
      "Published list to http://localhost:3000/browse2/stacey/pb_jlt_10/list/ArrowWeaveList/aa5b2a1badec3c01fa6f\n",
      "EVAL:  Attempt to solve the multiple choice math word problem by brainstorming a list of potential solutions and then systematically applying each one to the problem to determine if any progress can be achieved.\n",
      "SCORES:  [0, 0, 0, 0, 1]\n",
      "Published list to http://localhost:3000/browse2/stacey/pb_jlt_10/list/ArrowWeaveList/01823f466708c28bb906\n"
     ]
    }
   ],
   "source": [
    "PB = seed_PB({}, NUM_SEEDS)\n",
    "results = evolve(PB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220a6612-efd6-4fd5-b5f0-ca56f994d4e9",
   "metadata": {},
   "source": [
    "# 5. View lineage\n",
    "\n",
    "This is a good way to see the history of prompts.\n",
    "Note: with Hypermutate, we get a bunch more short lineages instead of longer individual lineages. interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2d69e4-c1ef-47b8-a5dd-c9bb1b0e6c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in PB.values():\n",
    "  print(\"winning prompt: \", p[\"prompt\"])\n",
    "  if \"lineage\" in p:\n",
    "    for i, l in enumerate(p[\"lineage\"]):\n",
    "      print(\"\\n\\n\\nAncestor \", i, \": \", l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9673a4f-5850-4e14-b5a2-add57b4cdf81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
