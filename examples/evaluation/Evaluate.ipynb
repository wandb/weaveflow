{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38eea4b-59f6-4b57-a40b-4d81e8566907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import weave\n",
    "from weave import weaveflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6020e0-3348-4ab8-b3cb-d68e5b870e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.init('eval-testing11')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b093c1-686f-45f9-b7cd-ac7b40cf99fe",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ef2842-cd79-4705-9473-9bff0465541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = weaveflow.Dataset([\n",
    "    {'id': '0', 'q': '1 + 1', 'a': '2'},\n",
    "    {'id': '1', 'q': '1 / 3', 'a': '0.3333333333'}])\n",
    "dataset_ref = weave.publish(dataset, 'dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562bd3bd-2d1c-471e-88b9-fe9208b6747a",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab3e775-bfe8-4f65-8fe5-92f568ff7410",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.type()\n",
    "class AddModel(weaveflow.Model):\n",
    "    llm: weaveflow.ChatModel\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, example: typing.Any) -> typing.Any:\n",
    "        response = self.llm.complete([{'role': 'user', 'content': 'Answer the following. Just provide the answer: ' + example['q']}])\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63233e8e-d798-449c-9c4b-9183d6dc6b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AddModel(weaveflow.OpenaiChatModel('gpt-3.5-turbo', 0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe8dfce-fd88-42da-bb9c-0a1c90194f76",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea24f2eb-36c9-4618-93e0-777c6f9ab196",
   "metadata": {},
   "source": [
    "Declare an exact match evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b271130-371f-48fa-ba0d-25437c263db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def get_answer_from_example(example: typing.Any) -> typing.Any:\n",
    "    return example['a']\n",
    "\n",
    "exact_evaluator = weaveflow.EvaluateExactMatch(get_answer_from_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd886e40-2d79-49cc-99d0-45a225b6d024",
   "metadata": {},
   "source": [
    "Declare an LLM evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e669ed-8f9b-489e-8743-501292e791b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def make_llm_eval_messages(example: typing.Any, prediction: typing.Any) -> typing.Any:\n",
    "    prompt_args = {\n",
    "        'question': example['q'],\n",
    "        'answer': example['a'],\n",
    "        'prediction': prediction\n",
    "    }\n",
    "    prompt = \"\"\"\n",
    "Please score the following, on a scale of 1-5, with one being worse. Also provide your rationale.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Correct answer: {prediction}\n",
    "\"\"\".format(**prompt_args)\n",
    "    return [{'role': 'user', 'content': prompt}]\n",
    "\n",
    "eval_chat_model = weaveflow.StructuredOutputChatModelSystemPrompt(\n",
    "    weaveflow.OpenaiChatModel('gpt-4', 0.7))\n",
    "\n",
    "llm_evaluator = weaveflow.EvaluateLLM(eval_chat_model, make_llm_eval_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8f9e3c-8c72-4551-a0b0-64b6e69c7c51",
   "metadata": {},
   "source": [
    "Make the model eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f691134-e80b-40f5-a160-4d78c9c1be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = weaveflow.EvaluateMulti({\n",
    "    'exact': exact_evaluator,\n",
    "    'llm': llm_evaluator\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66c723d-076a-4999-93d3-63776909d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "weaveflow.evaluate(evaluator, dataset_ref, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
